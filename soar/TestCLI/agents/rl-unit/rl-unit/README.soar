# This test covers the rl-rule sequence cases to behaviorally test
# the Soar-RL update mechanism.  Learning/discount-rate are set
# to known values and other parameters can be tweaked.  Some of the
# expected behavior is documented below.

# Learning Rate: 0.1
# Discount Rate: 0.5
# Reward: 100/decision on ALL states

####################################################
# Case 1: RL, RL (two RL rules in sequence)
#
# We expect the second rule to converge at reward.
# The first rule should converge at reward plus
# discount rate times reward.
#
# Updates for the first rule should be...
# update = (learning rate)*[ reward + discount*(second rule value) - (first rule value) ]
#
# Updates for the second rule should be...
# update = (learning rate)*[ reward + discount*(0) - (second rule value) ] = (learning rate)*[ reward - (second rule value) ]
#
# Temporal extensions (i.e. gaps) and HRL won't have any effect in this case.
#
# Value sequence (given parameters above):
# - rule one:   10, 19.5, 28.5, 37.005, 45.024 ... 150
# - rule two:   10, 19, 27.1, 34.39, 40.951 ... 100
#
####################################################


####################################################
# Case 2: RL, non-RL, RL (two RL rules separated by a single-decision gap)
#
# We expect the second rule to converge at reward.
# The first rule should converge at reward plus
# discount reward plus discount^2 reward.
#
# Updates for the first rule should be...
# update = (learning rate)*[ (reward + discount*reward) + (discount^2)*(second rule value) - (first rule value) ]
#
# Updates for the second rule should be...
# update = (learning rate)*[ reward + discount*(0) - (second rule value) ] = (learning rate)*[ reward - (second rule value) ]
#
# Temporal extensions (i.e. gaps) will cause the first rule to
# converge at reward (i.e. act identically to the second rule).
# HRL won't have any effect in this case.
#
# Value sequence (given parameters above):
# - rule one:   15, 28.75, 41.35, 52.8925, 63.463 ... 175
# - rule two:   10, 19, 27.1, 34.39, 40.951 ... 100
#
####################################################

####################################################
# Case 3: RL, subgoal, RL (two RL rules separated by a subgoal of a single cycle)
#
# We expect the same results as case 2 above.
#
# HRL will cause the first rule to converge at 3xreward.
#
####################################################

####################################################
# Case 4: RL, non-RL, subgoal, RL (two RL rules separated by a single-decision gap and a single decision subgoal)
#
# We expect the second rule to converge at reward.
# The first rule should converge at reward plus
# discount reward plus discount^2 reward plus
# discount^3 reward plus discount^4 reward.
#
# Updates for the first rule should be...
# update = (learning rate)*[ (reward + discount*reward + discount^2*reward + discount^3*reward) + (discount^4)*(second rule value) - (first rule value) ]
#
# Updates for the second rule should be...
# update = (learning rate)*[ reward + discount*(0) - (second rule value) ] = (learning rate)*[ reward - (second rule value) ]
#
# Temporal extensions (i.e. gaps) will cause the first rule to
# converge at reward (i.e. act identically to the second rule).
# HRL will cause the first rule to converge at (reward + discount*reward + discount^2( 2*reward ) + discount^3*reward)
#
# Value sequence (given parameters above):
# - rule one:   18.75, 35.6875, 50.9875, 64.808125, 77.29225 ... 193.75
# - rule two:   10, 19, 27.1, 34.39, 40.951 ... 100
#
####################################################


